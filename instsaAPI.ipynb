{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install instagrapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instagrapi import Client\n",
    "import pickle\n",
    "from math import inf as Inf\n",
    "\n",
    "### GETTING THAT DATA\n",
    "\n",
    "cl = Client()\n",
    "\n",
    "\n",
    "USERNAME = \"insta_username\";\n",
    "PASSWORD = \"insta_password\";\n",
    "cl.login(USERNAME, PASSWORD) \n",
    "media_id = cl.media_id(cl.media_pk_from_url('https://www.instagram.com/p/CwJVnryNvxW/'))\n",
    "\n",
    "# Change the second argument to contron the number of comments to extract from the video (in this case it is 20)\n",
    "comments = cl.media_comments(media_id, 20)\n",
    "\n",
    "with open('comments.pkl', 'wb') as f:\n",
    "    pickle.dump(comments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 19\n"
     ]
    }
   ],
   "source": [
    "with open('comments.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "\n",
    "# Convert to pandas dataframe for easy manipulation\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(loaded_dict)\n",
    "print(\"number of rows:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "first_row = df.iloc[0].to_list()\n",
    "column_names = {}\n",
    "for i, name in enumerate(df.columns):\n",
    "    column_names[name] = first_row[i][0]\n",
    "\n",
    "# Renamed the column headers to their actual meanings\n",
    "df = df.rename(columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Config', '__abstractmethods__', '__annotations__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_get_value', '_init_private_attributes', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'full_name', 'is_private', 'json', 'parse_file', 'parse_obj', 'parse_raw', 'pk', 'profile_pic_url', 'profile_pic_url_hd', 'schema', 'schema_json', 'stories', 'update_forward_refs', 'username', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# Remove the headers in each data cell\n",
    "for i, row in df.iterrows():\n",
    "    for col in df.columns:\n",
    "        df.at[i, col] = row[col][1]\n",
    "\n",
    "print(dir(df.at[1, 'user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>ayesha__2314</td>\n",
       "      <td>2023-08-28 06:28:51+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beshak</td>\n",
       "      <td>samia.javed.9465</td>\n",
       "      <td>2023-08-26 14:55:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Excatly</td>\n",
       "      <td>loyal_rajpoot_17866</td>\n",
       "      <td>2023-08-25 17:31:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Follow for more</td>\n",
       "      <td>raeesul_official</td>\n",
       "      <td>2023-08-24 15:15:24+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baishak ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>h.mussab</td>\n",
       "      <td>2023-08-24 13:39:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beshak</td>\n",
       "      <td>tasawarpathan786</td>\n",
       "      <td>2023-08-24 05:49:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beshak</td>\n",
       "      <td>miralmiral9070</td>\n",
       "      <td>2023-08-23 18:00:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>rajputnaira30</td>\n",
       "      <td>2023-08-22 15:33:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beshk</td>\n",
       "      <td>kamal_khan5340</td>\n",
       "      <td>2023-08-22 14:52:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beshak</td>\n",
       "      <td>candajutt</td>\n",
       "      <td>2023-08-22 11:34:19+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beashak subhan allah</td>\n",
       "      <td>munzamamin</td>\n",
       "      <td>2023-08-21 18:05:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>üëçüëçüëçüëå</td>\n",
       "      <td>farzanaa_shafqat</td>\n",
       "      <td>2023-08-21 10:42:25+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subhan Allah bashak bashak ‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>abbassyedaliabuzar</td>\n",
       "      <td>2023-08-21 10:31:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>€åÿßÿ≠ÿ∂ÿ±ÿ™ ÿπŸÑ€å‚ù§Ô∏èüôå</td>\n",
       "      <td>zadehmehrdadrahim</td>\n",
       "      <td>2023-08-21 01:38:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SachiBaat</td>\n",
       "      <td>saeed.batish.7</td>\n",
       "      <td>2023-08-20 10:44:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hazrat Ali alaihissalam</td>\n",
       "      <td>wasiali_21</td>\n",
       "      <td>2023-08-20 09:49:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Beshaqüî•üî•üî•</td>\n",
       "      <td>abdul__ismail__mewati</td>\n",
       "      <td>2023-08-20 09:29:37+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>üëçüëçüëçüíØüíØ beshak</td>\n",
       "      <td>salma_hansari_</td>\n",
       "      <td>2023-08-20 04:36:03+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Beshak üíØüëåüëåüëå</td>\n",
       "      <td>suhanakhatun3069</td>\n",
       "      <td>2023-08-20 00:50:02+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text               username  \\\n",
       "0                            ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è           ayesha__2314   \n",
       "1                            Beshak       samia.javed.9465   \n",
       "2                           Excatly    loyal_rajpoot_17866   \n",
       "3                   Follow for more       raeesul_official   \n",
       "4                    Baishak ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è               h.mussab   \n",
       "5                            Beshak       tasawarpathan786   \n",
       "6                            Beshak         miralmiral9070   \n",
       "7                          ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è          rajputnaira30   \n",
       "8                             Beshk         kamal_khan5340   \n",
       "9                            Beshak              candajutt   \n",
       "10             Beashak subhan allah             munzamamin   \n",
       "11                             üëçüëçüëçüëå       farzanaa_shafqat   \n",
       "12  Subhan Allah bashak bashak ‚ù§Ô∏è‚ù§Ô∏è     abbassyedaliabuzar   \n",
       "13                    €åÿßÿ≠ÿ∂ÿ±ÿ™ ÿπŸÑ€å‚ù§Ô∏èüôå      zadehmehrdadrahim   \n",
       "14                        SachiBaat         saeed.batish.7   \n",
       "15          Hazrat Ali alaihissalam             wasiali_21   \n",
       "16                        Beshaqüî•üî•üî•  abdul__ismail__mewati   \n",
       "17                     üëçüëçüëçüíØüíØ beshak         salma_hansari_   \n",
       "18                      Beshak üíØüëåüëåüëå       suhanakhatun3069   \n",
       "\n",
       "               created_at_utc  \n",
       "0   2023-08-28 06:28:51+00:00  \n",
       "1   2023-08-26 14:55:26+00:00  \n",
       "2   2023-08-25 17:31:31+00:00  \n",
       "3   2023-08-24 15:15:24+00:00  \n",
       "4   2023-08-24 13:39:45+00:00  \n",
       "5   2023-08-24 05:49:23+00:00  \n",
       "6   2023-08-23 18:00:42+00:00  \n",
       "7   2023-08-22 15:33:08+00:00  \n",
       "8   2023-08-22 14:52:04+00:00  \n",
       "9   2023-08-22 11:34:19+00:00  \n",
       "10  2023-08-21 18:05:42+00:00  \n",
       "11  2023-08-21 10:42:25+00:00  \n",
       "12  2023-08-21 10:31:21+00:00  \n",
       "13  2023-08-21 01:38:21+00:00  \n",
       "14  2023-08-20 10:44:12+00:00  \n",
       "15  2023-08-20 09:49:50+00:00  \n",
       "16  2023-08-20 09:29:37+00:00  \n",
       "17  2023-08-20 04:36:03+00:00  \n",
       "18  2023-08-20 00:50:02+00:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Special edit to convert user > username\n",
    "for i, row in df.iterrows():\n",
    "    placeholder = row['user']\n",
    "    df.at[i, 'user'] = placeholder.username\n",
    "    df.at[i, 'user_id'] = placeholder.pk\n",
    "df = df.rename(columns={'user': 'username'})\n",
    "# print(df.head())\n",
    "\n",
    "# I just need the text, username and timestamp\n",
    "new_df = df.loc[:, ['text', 'username', 'created_at_utc']]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social_Media_BOTS-main\\veSSA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output.logits[0].detach().numpy()\n",
    "    scores = softmax(scores, axis=0)\n",
    "\n",
    "    # Labels corresponding to sentiment classes\n",
    "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "    json_sent = {\n",
    "        \"label\": labels[np.argmax(scores)],\n",
    "        \"probability\": {\n",
    "            \"neg\": scores[0],  # Negative sentiment score\n",
    "            \"neutral\": scores[1],  # Neutral sentiment score\n",
    "            \"pos\": scores[2]  # Positive sentiment score\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return json_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textDisplay     label       pos       neg   neutral\n",
      "0           ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è  positive  0.944387  0.007543  0.048070\n",
      "1           Beshak   neutral  0.207550  0.117964  0.674486\n",
      "2          Excatly   neutral  0.226173  0.073018  0.700809\n",
      "3  Follow for more   neutral  0.331225  0.023652  0.645123\n",
      "4   Baishak ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è  positive  0.956012  0.003340  0.040648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Assuming you already have 'new_df' with the comments\n",
    "\n",
    "# Create a new DataFrame to store sentiment analysis results\n",
    "result_df = pd.DataFrame(columns=['textDisplay', 'label', 'pos', 'neg', 'neutral'])\n",
    "\n",
    "# Iterate through each row in the original DataFrame\n",
    "for index, row in new_df.iterrows():\n",
    "    comment = row['text']\n",
    "    \n",
    "    # Analyze sentiment for the comment using the analyze_sentiment function\n",
    "    sentiment_result = analyze_sentiment(comment)\n",
    "    \n",
    "    # Add sentiment analysis results to the new DataFrame\n",
    "    result_df.loc[index] = [\n",
    "        comment,\n",
    "        sentiment_result['label'],\n",
    "        sentiment_result['probability']['pos'],\n",
    "        sentiment_result['probability']['neg'],\n",
    "        sentiment_result['probability']['neutral']\n",
    "    ]\n",
    "\n",
    "# Print the resulting DataFrame with sentiment analysis results\n",
    "print(result_df.head())  # Print the first 5 entries\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('sentiment_analysis_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the overall sentiment of the comment section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_avg_pos  0.5563612\n",
      "weighted_avg_neg  0.05886369\n",
      "weighted_avg_neutral  0.38477513\n",
      "max_weighted_avg  0.5563612\n",
      "The Overall determined sentiment label is: positive\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average for pos, neg, and neutral columns\n",
    "weighted_avg_pos = result_df['pos'].mean()\n",
    "print(\"weighted_avg_pos \", weighted_avg_pos)\n",
    "weighted_avg_neg = result_df['neg'].mean()\n",
    "print(\"weighted_avg_neg \", weighted_avg_neg)\n",
    "\n",
    "weighted_avg_neutral = result_df['neutral'].mean()\n",
    "print(\"weighted_avg_neutral \", weighted_avg_neutral)\n",
    "\n",
    "\n",
    "# Determine the label based on the greatest weighted average\n",
    "max_weighted_avg = max(weighted_avg_pos, weighted_avg_neg, weighted_avg_neutral)\n",
    "print(\"max_weighted_avg \",max_weighted_avg)\n",
    "label = None\n",
    "if max_weighted_avg == weighted_avg_pos:\n",
    "    label = 'positive'\n",
    "elif max_weighted_avg == weighted_avg_neg:\n",
    "    label = 'negative'\n",
    "else:\n",
    "    label = 'neutral'\n",
    "\n",
    "# Print the determined label\n",
    "print(f\"The Overall determined sentiment label is: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the absolute number of positive/negative/neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Comments: 19\n",
      "Number of Positive Comments out of 19 = 9\n",
      "Number of Negative Comments out of 19 = 0\n",
      "Number of Neutral Comments out of 19 = 10\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of comments\n",
    "total_comments = len(result_df)\n",
    "\n",
    "# Count the number of positive, negative, and neutral comments\n",
    "num_positive_comments = result_df[result_df['label'] == 'positive']['textDisplay'].count()\n",
    "num_negative_comments = result_df[result_df['label'] == 'negative']['textDisplay'].count()\n",
    "num_neutral_comments = result_df[result_df['label'] == 'neutral']['textDisplay'].count()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Comments: {total_comments}\")\n",
    "print(f\"Number of Positive Comments out of {total_comments} = {num_positive_comments}\")\n",
    "print(f\"Number of Negative Comments out of {total_comments} = {num_negative_comments}\")\n",
    "print(f\"Number of Neutral Comments out of {total_comments} = {num_neutral_comments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the percentage of positive/negative/neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Positive Comments: 47.37%\n",
      "Percentage of Negative Comments: 0.00%\n",
      "Percentage of Neutral Comments: 52.63%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentages of positive, negative, and neutral comments\n",
    "percentage_positive = (num_positive_comments / total_comments) * 100\n",
    "percentage_negative = (num_negative_comments / total_comments) * 100\n",
    "percentage_neutral = (num_neutral_comments / total_comments) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Percentage of Positive Comments: {percentage_positive:.2f}%\")\n",
    "print(f\"Percentage of Negative Comments: {percentage_negative:.2f}%\")\n",
    "print(f\"Percentage of Neutral Comments: {percentage_neutral:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common words that appeared in the positive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è', 2),\n",
       " ('bashak', 2),\n",
       " ('beshak', 2),\n",
       " ('baishak', 1),\n",
       " ('‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è', 1),\n",
       " ('üëçüëçüëçüëå', 1),\n",
       " ('subhan', 1),\n",
       " ('allah', 1),\n",
       " ('‚ù§Ô∏è‚ù§Ô∏è', 1),\n",
       " ('€åÿßÿ≠ÿ∂ÿ±ÿ™', 1),\n",
       " ('ÿπŸÑ€å‚ù§Ô∏èüôå', 1),\n",
       " ('beshaqüî•üî•üî•', 1),\n",
       " ('üëçüëçüëçüíØüíØ', 1),\n",
       " ('üíØüëåüëåüëå', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import operator\n",
    "\n",
    "import re\n",
    "#the words that appear he most in positive reviews\n",
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "list_pos=[]\n",
    "for i in range(len(result_df.loc[result_df['label'] == 'positive'])):\n",
    "    list_pos.append(result_df.loc[result_df['label'] == 'positive'][\"textDisplay\"].iloc[i])\n",
    "lst_words_pos = []\n",
    "for line in list_pos:\n",
    "    text_pos = re.split('\\n| |\\?|\\!|\\:|\\\"|\\(|\\)|\\...|\\;',line)\n",
    "    for word in text_pos:\n",
    "        if (len(word)>3 and not word.startswith('@') and not word.startswith('#') and word != 'RT'):\n",
    "            lst_words_pos.append(porter.stem(word.lower()))\n",
    "\n",
    "\n",
    "dist_pos = FreqDist(lst_words_pos) \n",
    "sorted_dist_pos = sorted(dist_pos.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_dist_pos[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A list of the common words that appeared in the negative comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_neg=[]\n",
    "for i in range(len(result_df.loc[result_df['label'] == 'negative'])):\n",
    "    list_neg.append(result_df.loc[result_df['label'] == 'negative'][\"textDisplay\"].iloc[i])\n",
    "lst_words_neg = []\n",
    "for line in list_neg:\n",
    "    text_neg = re.split('\\n| |\\?|\\!|\\:|\\\"|\\(|\\)|\\...|\\;',line)\n",
    "    for word in text_neg:\n",
    "        if (len(word)>3 and not word.startswith('@') and not word.startswith('#') and word != 'RT'):\n",
    "            lst_words_neg.append(porter.stem(word.lower()))\n",
    "dist_neg = FreqDist(lst_words_neg) \n",
    "sorted_dist_neg = sorted(dist_neg.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_dist_neg[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
